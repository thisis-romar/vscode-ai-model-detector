"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || (function () {
    var ownKeys = function(o) {
        ownKeys = Object.getOwnPropertyNames || function (o) {
            var ar = [];
            for (var k in o) if (Object.prototype.hasOwnProperty.call(o, k)) ar[ar.length] = k;
            return ar;
        };
        return ownKeys(o);
    };
    return function (mod) {
        if (mod && mod.__esModule) return mod;
        var result = {};
        if (mod != null) for (var k = ownKeys(mod), i = 0; i < k.length; i++) if (k[i] !== "default") __createBinding(result, mod, k[i]);
        __setModuleDefault(result, mod);
        return result;
    };
})();
Object.defineProperty(exports, "__esModule", { value: true });
exports.ChatParticipantHandler = void 0;
const vscode = __importStar(require("vscode"));
/**
 * Chat participant for real-time AI model detection with 100% accuracy
 * Enhanced with MCP integration via IPC bridge
 */
class ChatParticipantHandler {
    constructor(detectorService, ipcBridge) {
        this.detectorService = detectorService;
        this.ipcBridge = ipcBridge;
        console.log('üéØ Chat Participant initialized with', ipcBridge ? 'MCP bridge' : 'direct API only');
    }
    /**
     * Handle chat requests for @modeldetector participant
     */
    async handleChatRequest(request, context, stream, token) {
        // Handle different commands
        if (request.command === 'detect') {
            return this.handleDetectCommand(request, stream, token);
        }
        else if (request.command === 'monitor') {
            return this.handleMonitorCommand(request, stream, token);
        }
        else {
            // Default: comprehensive model detection
            return this.handleDefaultDetection(request, stream, token);
        }
    }
    /**
     * Handle /detect command for comprehensive model information
     */
    async handleDetectCommand(request, stream, token) {
        stream.progress('üîç Detecting current AI model with 100% accuracy...');
        try {
            // üéØ PRIMARY METHOD: 100% accurate detection via chat context
            const result = await this.detectorService.detectFromChatContext(request);
            if (result.success && result.model) {
                await this.streamModelInformation(stream, result.model, result);
                // Also show available models as context
                const lmResult = await this.detectorService.detectFromLMAPI();
                if (lmResult.availableModels && lmResult.availableModels.length > 1) {
                    await this.streamAvailableModels(stream, lmResult.availableModels);
                }
                return {
                    metadata: {
                        command: 'detect',
                        modelDetected: true,
                        accuracy: '100%'
                    }
                };
            }
            else {
                stream.markdown(`## ‚ùå Detection Failed\n\n`);
                stream.markdown(`**Error:** ${result.error}\n\n`);
                stream.markdown(`Please ensure you're using a supported AI model in VS Code chat.\n`);
                return {
                    metadata: {
                        command: 'detect',
                        modelDetected: false,
                        accuracy: 'failed'
                    }
                };
            }
        }
        catch (error) {
            stream.markdown(`## ‚ùå Unexpected Error\n\n`);
            stream.markdown(`**Error:** ${error instanceof Error ? error.message : 'Unknown error'}\n`);
            return {
                metadata: {
                    command: 'detect',
                    modelDetected: false,
                    accuracy: 'error'
                }
            };
        }
    }
    /**
     * Handle /monitor command for continuous monitoring setup
     */
    async handleMonitorCommand(request, stream, token) {
        stream.progress('‚öôÔ∏è Setting up continuous model monitoring...');
        // Enable status bar monitoring
        const config = vscode.workspace.getConfiguration('aiModelDetector');
        await config.update('enableStatusBar', true, vscode.ConfigurationTarget.Global);
        stream.markdown(`## ‚öôÔ∏è Monitoring Enabled\n\n`);
        stream.markdown(`‚úÖ **Status bar monitoring** is now active\n`);
        stream.markdown(`‚úÖ **Real-time model detection** enabled\n\n`);
        stream.markdown(`### üìä Monitoring Features:\n\n`);
        stream.markdown(`- Status bar shows current model continuously\n`);
        stream.markdown(`- Click status bar item for detailed information\n`);
        stream.markdown(`- Automatic updates when model changes\n`);
        stream.markdown(`- Detection history tracking\n\n`);
        // Show current model
        const result = await this.detectorService.detectFromChatContext(request);
        if (result.success && result.model) {
            stream.markdown(`### üéØ Currently Detected Model:\n\n`);
            stream.markdown(`**${result.model.name}** (${result.model.vendor})\n`);
        }
        // Add command buttons
        stream.button({
            command: 'aiModelDetector.detect',
            title: 'üîç Manual Detection'
        });
        stream.button({
            command: 'aiModelDetector.showHistory',
            title: 'üìã Show History'
        });
        return {
            metadata: {
                command: 'monitor',
                modelDetected: true,
                accuracy: 'monitoring-enabled'
            }
        };
    }
    /**
     * Handle default detection (no specific command)
     */
    async handleDefaultDetection(request, stream, token) {
        stream.progress('üéØ Analyzing current AI model selection...');
        try {
            // Perform real-time detection
            const result = await this.detectorService.detectFromChatContext(request);
            if (result.success && result.model) {
                // Stream comprehensive model information
                await this.streamModelInformation(stream, result.model, result);
                // Add helpful buttons
                stream.button({
                    command: 'aiModelDetector.toggleStatusBar',
                    title: 'üìä Toggle Status Bar'
                });
                stream.button({
                    command: 'aiModelDetector.showHistory',
                    title: 'üìã Detection History'
                });
                return {
                    metadata: {
                        command: 'default',
                        modelDetected: true,
                        accuracy: '100%'
                    }
                };
            }
            else {
                // Show fallback information
                stream.markdown(`## ‚ö†Ô∏è Limited Detection\n\n`);
                stream.markdown(`Chat context detection failed, trying alternative methods...\n\n`);
                const fallbackResult = await this.detectorService.detectCurrentModel();
                if (fallbackResult.success && fallbackResult.model) {
                    await this.streamModelInformation(stream, fallbackResult.model, fallbackResult);
                }
                else {
                    stream.markdown(`‚ùå **All detection methods failed**\n\n`);
                    stream.markdown(`${fallbackResult.error}\n`);
                }
                return {
                    metadata: {
                        command: 'default',
                        modelDetected: fallbackResult.success,
                        accuracy: 'fallback'
                    }
                };
            }
        }
        catch (error) {
            stream.markdown(`## ‚ùå Detection Error\n\n`);
            stream.markdown(`**Error:** ${error instanceof Error ? error.message : 'Unknown error'}\n`);
            return {
                metadata: {
                    command: 'default',
                    modelDetected: false,
                    accuracy: 'error'
                }
            };
        }
    }
    /**
     * Stream comprehensive model information to chat
     */
    async streamModelInformation(stream, model, result) {
        stream.markdown(`## üéØ AI Model Detection Results\n\n`);
        // Accuracy indicator
        const accuracyEmoji = model.accuracy === 'Perfect' ? '‚úÖ' :
            model.accuracy === 'Available' ? 'üîç' :
                model.accuracy === 'Cached' ? 'üíæ' : '‚ùì';
        stream.markdown(`${accuracyEmoji} **Accuracy:** ${model.accuracy} (${model.source})\n\n`);
        // Model details table
        stream.markdown(`### üìã Model Details\n\n`);
        stream.markdown(`| Property | Value |\n`);
        stream.markdown(`|----------|-------|\n`);
        stream.markdown(`| **Model ID** | \`${model.id}\` |\n`);
        stream.markdown(`| **Display Name** | \`${model.name}\` |\n`);
        stream.markdown(`| **Vendor** | \`${model.vendor}\` |\n`);
        stream.markdown(`| **Family** | \`${model.family}\` |\n`);
        stream.markdown(`| **Version** | \`${model.version}\` |\n`);
        stream.markdown(`| **Max Tokens** | \`${model.maxTokens.toLocaleString()}\` |\n\n`);
        // Performance metrics
        if (result.performance) {
            stream.markdown(`### ‚ö° Performance\n\n`);
            stream.markdown(`- **Detection Time:** ${result.performance.duration}ms\n`);
            stream.markdown(`- **Method:** ${result.method}\n`);
            stream.markdown(`- **Timestamp:** ${model.timestamp.toISOString()}\n\n`);
        }
        // Technical details
        if (model.metadata) {
            stream.markdown(`### üîß Technical Details\n\n`);
            stream.markdown(`\`\`\`json\n${JSON.stringify(model.metadata, null, 2)}\n\`\`\`\n\n`);
        }
    }
    /**
     * Stream available models information
     */
    async streamAvailableModels(stream, models) {
        stream.markdown(`### üîç Available Models\n\n`);
        for (const model of models) {
            const isCurrentEmoji = 'üéØ'; // Could be enhanced to detect current
            stream.markdown(`- ${isCurrentEmoji} **${model.name}** (\`${model.id}\`) - ${model.vendor}\n`);
        }
        stream.markdown(`\n*Total available models: ${models.length}*\n\n`);
    }
}
exports.ChatParticipantHandler = ChatParticipantHandler;
//# sourceMappingURL=chatParticipant.js.map